{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6950d686-4d41-41c5-84e2-b7246c76984f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e593956-6eaa-4234-878e-cd0745209f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from params import GENRES_SEQ_LEN, TAGS_SEQ_LEN\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a802ebc4-7d3f-4859-adf3-9a5e8a6de65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = torch.load(\"run_artifacts/preprocess/dataset.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887cec5a-5cad-41a6-bb0e-813c0ce93dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"run_artifacts/preprocess/mappings.json\", \"r\") as f:\n",
    "    MAPPINGS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821ca5b9-6510-464b-bc3c-38069673fcba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_test_indices = torch.randperm(len(dataset))\n",
    "train_size = 500\n",
    "train_indices = train_test_indices[:train_size]\n",
    "test_indices = train_test_indices[train_size:]\n",
    "\n",
    "X_train, y_train = dataset[train_indices]\n",
    "X_test, y_test = dataset[test_indices]\n",
    "\n",
    "# The goal is predicting how much the items in test set are similar to train set\n",
    "y_train = y_train[:, train_indices]\n",
    "y_test = y_test[:, train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83ecf05-951d-4505-b02e-34e47abbbbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert boolans to floats\n",
    "edge_threshold = 0.01\n",
    "y_train = y_train > edge_threshold\n",
    "y_test = y_test > edge_threshold\n",
    "y_train, y_test = y_train.float(), y_test.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d96ba0-bd0d-4e9b-9921-2f11008871b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6001aad0-447e-427f-b29c-2f42656337d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018260415643453598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the positive rate is very low\n",
    "y_test.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca15cd8-15f4-4706-92d5-3b690eca936e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper for doing pack and unpacks in GRU\n",
    "class GRU_subunit(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, inputs_len):\n",
    "        self.gru.flatten_parameters()\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            inputs,\n",
    "            inputs_len.flatten().cpu().int(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        _, x = self.gru(x)  # returns output, final_hidden\n",
    "        x = x[-1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd075b0-5978-48f9-b0b8-c664595064dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.developer_emb_dim = 2  # dimension of the developer embedding\n",
    "        self.genres_emb_dim = 10  # dimension of the genres embedding\n",
    "        self.genres_out_dim = 10  # output dimension of the genres GRU layer\n",
    "        self.tags_emb_dim = 24  # dimension of the tags embedding\n",
    "        self.tags_out_dim = 24  # output dimension of the tags GRU layer\n",
    "\n",
    "        # Developer layers\n",
    "        self.developer_embedding = nn.Embedding(\n",
    "            len(MAPPINGS[\"developer\"]) + 1, self.developer_emb_dim\n",
    "        )\n",
    "\n",
    "        # Genres layers\n",
    "        self.genres_embedding = nn.Embedding(\n",
    "            len(MAPPINGS[\"Genres\"]) + 1, self.genres_emb_dim\n",
    "        )\n",
    "        self.genres_gru = GRU_subunit(self.genres_emb_dim, self.genres_out_dim)\n",
    "\n",
    "        # Tags layers\n",
    "        self.tags_embedding = nn.Embedding(len(MAPPINGS[\"Tags\"]) + 1, self.tags_emb_dim)\n",
    "        self.tags_gru = GRU_subunit(self.tags_emb_dim, self.tags_out_dim)\n",
    "\n",
    "        # Batch normalization layer and linear layers for concatenating the inputs and generating output\n",
    "        # size(1) = 1(price) + developer + genres + tags + tfidf-fasttext embedding\n",
    "        self.concat_batchnorm = nn.BatchNorm1d(\n",
    "            1 + self.developer_emb_dim + self.genres_out_dim + self.tags_out_dim + 300\n",
    "        )\n",
    "        self.concat_linear_1 = nn.Linear(\n",
    "            1 + self.developer_emb_dim + self.genres_out_dim + self.tags_out_dim + 300,\n",
    "            32,\n",
    "        )\n",
    "        self.concat_relu = nn.ReLU()\n",
    "        self.concat_linear_out = nn.Linear(32, train_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Splitting the input tensor into its components\n",
    "        (\n",
    "            price,\n",
    "            developer,\n",
    "            genres_seq,\n",
    "            genres_len,\n",
    "            tags_seq,\n",
    "            tags_len,\n",
    "            weighted_embeddings,\n",
    "        ) = torch.split(x, [1, 1, GENRES_SEQ_LEN, 1, TAGS_SEQ_LEN, 1, 300], dim=1)\n",
    "\n",
    "        # Processing the developer input\n",
    "        developer = developer.long()\n",
    "        developer = self.developer_embedding(developer)\n",
    "        developer = developer[:, 0, :]\n",
    "\n",
    "        # Processing the genres input\n",
    "        genres = genres_seq.long()\n",
    "        genres = self.genres_embedding(genres)\n",
    "        genres = self.genres_gru(genres, genres_len)\n",
    "\n",
    "        # Processing the tags input\n",
    "        tags = tags_seq.long()\n",
    "        tags = self.tags_embedding(tags)\n",
    "        tags = self.tags_gru(tags, tags_len)\n",
    "\n",
    "        # Concatenating the inputs and passing through the linear layers\n",
    "        concat = torch.cat((price, developer, genres, tags, weighted_embeddings), 1)\n",
    "        concat = self.concat_batchnorm(concat)\n",
    "        concat = self.concat_linear_1(concat)\n",
    "        concat = self.concat_relu(concat)\n",
    "        concat = self.concat_linear_out(concat)\n",
    "\n",
    "        # Returning the output tensor\n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d420bfe4-a97e-479c-9ec1-f6300e7acae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_accuracy(predicted, target):\n",
    "    intersection = (predicted * target).sum(dim=1)\n",
    "    union = (predicted + target).clamp(0, 1).sum(dim=1)\n",
    "    jaccard_similarity = intersection / union\n",
    "    jaccard_similarity = torch.nan_to_num(jaccard_similarity, 0)\n",
    "    return jaccard_similarity.mean(), intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1094a93e-a99f-4e99-a4f6-6e3df61a5955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_pct(output_tensor, top_pct):\n",
    "    # Determine the number of probabilities to keep per row\n",
    "    num_keep = int(output_tensor.shape[1] * top_pct)\n",
    "\n",
    "    # Find the indices of the top probabilities per row\n",
    "    top_indices = torch.topk(output_tensor, num_keep, dim=1)[1]\n",
    "\n",
    "    # Create a mask to select the top probabilities per row\n",
    "    mask = torch.zeros_like(output_tensor)\n",
    "    mask.scatter_(1, top_indices, 1)\n",
    "\n",
    "    # Apply the mask to select the top probabilities per row\n",
    "    top_probs = output_tensor * mask\n",
    "\n",
    "    # Set prediciton = 0 if true\n",
    "    top_probs = (top_probs > 0).float()\n",
    "    return top_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9c0cce-129f-4067-9925-9329becd722b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the output positve rate be on average the train positive rate\n",
    "top_pct = y_train.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c3f695-8685-47f1-8ba7-0e238e999bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6335 Test Jaccard Similarity: 0.73%\n",
      "Epoch [6/50], Loss: 0.0994 Test Jaccard Similarity: 2.13%\n",
      "Epoch [11/50], Loss: 0.0980 Test Jaccard Similarity: 4.91%\n",
      "Epoch [16/50], Loss: 0.0758 Test Jaccard Similarity: 5.19%\n",
      "Epoch [21/50], Loss: 0.0604 Test Jaccard Similarity: 5.55%\n",
      "Epoch [26/50], Loss: 0.1102 Test Jaccard Similarity: 5.87%\n",
      "Epoch [31/50], Loss: 0.0811 Test Jaccard Similarity: 6.24%\n",
      "Epoch [36/50], Loss: 0.0629 Test Jaccard Similarity: 6.25%\n",
      "Epoch [41/50], Loss: 0.0621 Test Jaccard Similarity: 6.30%\n",
      "Epoch [46/50], Loss: 0.0410 Test Jaccard Similarity: 6.50%\n",
      "Epoch [50/50], Loss: 0.0648 Test Jaccard Similarity: 6.75%\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Create a random permutation of indices for the batch sampling\n",
    "    indices = torch.randperm(X_train.shape[0])\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Get the batch of data and labels\n",
    "        batch_indices = indices[i : i + batch_size]\n",
    "        batch_X, batch_y = X_train[batch_indices], y_train[batch_indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the progress\n",
    "    if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "        with torch.no_grad():\n",
    "            test_outputs = torch.sigmoid(model(X_test))\n",
    "            predictions = get_top_pct(test_outputs, top_pct)\n",
    "            accuracy, intersection, union = multi_label_accuracy(predictions, y_test)\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\",\n",
    "                f\"Test Jaccard Similarity: {accuracy * 100:.2f}%\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba4a7b2-6f2c-4818-b0e9-ee9282005bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0183, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce45275-ac26-4fb4-8426-19733ba8866d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0220, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb8ea3-fcfd-4e1d-b4b1-c191852bad25",
   "metadata": {},
   "source": [
    "While an average similarity of 6.6% is very poor, this can be due to multiple factors:\n",
    "- Our initial edge matrix has issues including its sparsity and the very low likelihood of the same user being in the recommendations dataset twice.\n",
    "- The positive label rate is very sparse (1.8%) so finding positive labels is equally difficult\n",
    "- Our training data is only 500 samples long but contains many features (dimension 300+)\n",
    "- The  training methods and hyperparameters could be further tuned once data issues are solved\n",
    "\n",
    "On the positive side, 6.6% is still about 3.5 times the average positive rate. A random guess (coin flip) would have a score of approximately 1.8%. And the loss is going down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b8ea2-8b9c-449f-9b3f-d6794e89fc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchGeometricTemporal",
   "language": "python",
   "name": "pygt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
